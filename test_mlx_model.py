#!/usr/bin/env python3
"""
Test script for MLX fine-tuned Kazakh Law Gemma model
"""

import os

def test_mlx_model():
    """Interactive testing for MLX model"""
    
    try:
        from mlx_lm import load, generate
        print("‚úÖ MLX –¥–æ—Å—Ç—É–ø–µ–Ω")
    except ImportError:
        print("‚ùå MLX –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install mlx-lm")
        return
    
    if not os.path.exists("./checkpoints"):
        print("‚ùå –ê–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python fine_tune_gemma_mlx.py")
        return
    
    print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    try:
        # Load the fine-tuned model with adapters
        model, tokenizer = load(
            "google/gemma-3-1b-it",
            adapter_path="./checkpoints"
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return
    
    print("\n" + "="*60)
    print("üèõÔ∏è  –ö–ê–ó–ê–•–°–ö–ò–ô –ü–†–ê–í–û–í–û–ô –ê–°–°–ò–°–¢–ï–ù–¢ (MLX)")
    print("="*60)
    print("–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–º—É –ø—Ä–∞–≤—É!")
    print("–ù–∞–ø–∏—à–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞\n")
    
    # Example questions
    examples = [
        "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¢–û–û –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ?",
        "–ö–∞–∫–æ–≤–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è —Ç—Ä—É–¥–æ–≤–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞ –≤ –†–ö?",
        "–ö–∞–∫–∏–µ –Ω–∞–ª–æ–≥–∏ –æ–±—è–∑–∞–Ω–æ –ø–ª–∞—Ç–∏—Ç—å –¢–û–û –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ?",
        "–ö–∞–∫–æ–≤–∞ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏–µ –Ω–∞–ª–æ–≥–æ–≤–æ–≥–æ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞?",
    ]
    
    print("–ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:")
    for i, q in enumerate(examples, 1):
        print(f"{i}. {q}")
    print()
    
    while True:
        question = input("üí¨ –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
        
        if question.lower() in ['exit', '–≤—ã—Ö–æ–¥', 'quit']:
            break
            
        if not question:
            continue
            
        print("\nü§î –î—É–º–∞—é...")
        
        try:
            # Format prompt for Gemma
            prompt = f"<bos><start_of_turn>user\n{question}<end_of_turn>\n<start_of_turn>model\n"
            
            # Generate response
            response = generate(
                model, 
                tokenizer, 
                prompt=prompt, 
                max_tokens=300
            )
            
            # Extract just the answer part
            if "<start_of_turn>model\n" in response:
                answer = response.split("<start_of_turn>model\n")[-1]
                if "<end_of_turn>" in answer:
                    answer = answer.split("<end_of_turn>")[0]
            else:
                answer = response
            
            print(f"\nüìú –û—Ç–≤–µ—Ç:\n{answer.strip()}\n")
            print("-" * 60)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
    
    print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üëã")

def benchmark_mlx():
    """Quick benchmark of MLX performance"""
    print("‚ö° –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ MLX...")
    
    try:
        from mlx_lm import load, generate
        import time
        
        model, tokenizer = load(
            "google/gemma-3-1b-it",
            adapter_path="./checkpoints" if os.path.exists("./checkpoints") else None
        )
        
        test_prompt = "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¢–û–û?"
        prompt = f"<bos><start_of_turn>user\n{test_prompt}<end_of_turn>\n<start_of_turn>model\n"
        
        # Warmup
        _ = generate(model, tokenizer, prompt=prompt, max_tokens=50)
        
        # Benchmark
        start_time = time.time()
        response = generate(model, tokenizer, prompt=prompt, max_tokens=100)
        end_time = time.time()
        
        # Calculate tokens per second (rough estimate)
        response_tokens = len(tokenizer.encode(response)) - len(tokenizer.encode(prompt))
        tokens_per_second = response_tokens / (end_time - start_time)
        
        print(f"üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: ~{tokens_per_second:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {end_time - start_time:.2f} —Å–µ–∫")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "benchmark":
        benchmark_mlx()
    else:
        test_mlx_model() 