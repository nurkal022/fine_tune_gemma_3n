#!/usr/bin/env python3
"""
Quick test of the fine-tuned MLX model
"""

def quick_test():
    try:
        from mlx_lm import load, generate
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        
        model, tokenizer = load(
            "google/gemma-3-1b-it",
            adapter_path="./checkpoints"
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        
        # Test questions
        questions = [
            "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¢–û–û –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ?",
            "–ö–∞–∫–æ–≤–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è —Ç—Ä—É–¥–æ–≤–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞ –≤ –†–ö?",
            "–ö–∞–∫–∏–µ –Ω–∞–ª–æ–≥–∏ –æ–±—è–∑–∞–Ω–æ –ø–ª–∞—Ç–∏—Ç—å –¢–û–û –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ?"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\n{i}. üìù –í–æ–ø—Ä–æ—Å: {question}")
            
            prompt = f"<bos><start_of_turn>user\n{question}<end_of_turn>\n<start_of_turn>model\n"
            
            try:
                response = generate(model, tokenizer, prompt=prompt, max_tokens=200)
                
                # Extract answer
                if "<start_of_turn>model\n" in response:
                    answer = response.split("<start_of_turn>model\n")[-1]
                    if "<end_of_turn>" in answer:
                        answer = answer.split("<end_of_turn>")[0]
                else:
                    answer = response
                
                print(f"ü§ñ –û—Ç–≤–µ—Ç: {answer.strip()}")
                print("-" * 60)
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                print("-" * 60)
    
    except ImportError:
        print("‚ùå MLX –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    quick_test() 