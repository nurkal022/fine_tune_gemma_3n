#!/usr/bin/env python3
"""
Fine-tune Gemma 3n E4B using MLX for Mac M1 Pro on full Kazakhstani law dataset
Based on: https://gist.github.com/alexweberk/635431b5c5773efd6d1755801020429f
"""

import json
import os
from pathlib import Path

def install_mlx():
    """Install MLX if not available"""
    try:
        import mlx.core as mx
        from mlx_lm import load, generate
        from mlx_lm.tuners.lora import LoRALinear
        print("‚úÖ MLX —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return True
    except ImportError:
        print("üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é MLX...")
        os.system("pip install mlx-lm")
        try:
            import mlx.core as mx
            from mlx_lm import load, generate
            print("‚úÖ MLX —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return True
        except ImportError:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å MLX")
            return False

def prepare_dataset():
    """Prepare dataset in MLX format"""
    print("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    # Load FULL dataset
    dataset_path = "final_training_dataset/kazakh_law_qa_full_20250702_013138.jsonl"
    
    # Create data directory
    os.makedirs("data", exist_ok=True)
    
    # Convert to MLX format
    with open(dataset_path, 'r', encoding='utf-8') as f:
        raw_data = [json.loads(line) for line in f]
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(raw_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    
    # Format for MLX training (ALL data)
    mlx_data = []
    for item in raw_data:  # Use ALL samples
        formatted = {
            "text": f"<bos><start_of_turn>user\n{item['instruction']}<end_of_turn>\n<start_of_turn>model\n{item['output']}<end_of_turn><eos>"
        }
        mlx_data.append(formatted)
    
    # Save train and valid sets
    train_size = int(len(mlx_data) * 0.9)
    train_data = mlx_data[:train_size]
    valid_data = mlx_data[train_size:]
    
    with open("data/train.jsonl", 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    with open("data/valid.jsonl", 'w', encoding='utf-8') as f:
        for item in valid_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(train_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, {len(valid_data)} –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")

def run_mlx_training():
    """Run MLX LoRA training"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å MLX...")
    
    MODEL_NAME = "google/gemma-3-4b-it"  # Gemma 3 4B - —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è (4.3GB)
    DATA_PATH = "data"  # MLX –æ–∂–∏–¥–∞–µ—Ç –ø–∞–ø–∫—É —Å train.jsonl/valid.jsonl
    BATCH_SIZE = 1
    GRAD_CHECKPOINT = True
    ITERS = 1000
    SAVE_EVERY = 100
    LEARNING_RATE = 2e-4
    
    train_cmd = f"TOKENIZERS_PARALLELISM=false python -m mlx_lm.lora " \
                f"--model \"{MODEL_NAME}\" " \
                f"--train " \
                f"--iters {ITERS} " \
                f"--data {DATA_PATH} " \
                f"--adapter-path ./checkpoints " \
                f"--save-every {SAVE_EVERY} " \
                f"--batch-size {BATCH_SIZE} " \
                f"--grad-checkpoint " \
                f"--learning-rate {LEARNING_RATE} "
    print(f"–í—ã–ø–æ–ª–Ω—è—é –∫–æ–º–∞–Ω–¥—É:\n\n{train_cmd}\n")
    os.system(train_cmd)

def test_model():
    """Test the fine-tuned model"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    try:
        from mlx_lm import load, generate
        
        # Load the fine-tuned model
        model, tokenizer = load(
            "google/gemma-3n-E4B-it",
            adapter_path="./checkpoints"
        )
        
        # Test question
        test_prompt = "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¢–û–û –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ?"
        prompt = f"<bos><start_of_turn>user\n{test_prompt}<end_of_turn>\n<start_of_turn>model\n"
        
        response = generate(model, tokenizer, prompt=prompt, max_tokens=256)
        
        print(f"\nüìù –í–æ–ø—Ä–æ—Å: {test_prompt}")
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {response}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

def main():
    """Main function"""
    print("üèõÔ∏è –î–û–û–ë–£–ß–ï–ù–ò–ï GEMMA 3n E4B –° MLX")
    print("=" * 50)
    
    # Check MLX installation
    if not install_mlx():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å MLX. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±—ã—á–Ω—ã–π —Å–∫—Ä–∏–ø—Ç.")
        return
    
    # Prepare dataset
    prepare_dataset()
    
    # Create checkpoints directory
    os.makedirs("checkpoints", exist_ok=True)
    
    # Run training
    run_mlx_training()
    
    # Test model
    test_model()
    
    print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("üìÅ –ê–¥–∞–ø—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./checkpoints/")
    print("üß™ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ test_mlx_model.py –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")

if __name__ == "__main__":
    main() 