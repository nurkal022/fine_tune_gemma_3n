#!/usr/bin/env python3
"""
Full training script for Gemma 3 4B on complete Kazakhstani law dataset
"""

import os
import json
import subprocess
import sys
from sklearn.model_selection import train_test_split

def prepare_full_dataset():
    """Prepare complete dataset for training"""
    print("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ü–û–õ–ù–û–ì–û –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    # Load complete dataset
    dataset_path = "final_training_dataset/kazakh_law_qa_full_20250702_013138.jsonl"
    
    if not os.path.exists(dataset_path):
        print(f"‚ùå –§–∞–π–ª {dataset_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return False
    
    # Create data directory
    os.makedirs("data", exist_ok=True)
    
    # Load all data
    with open(dataset_path, 'r', encoding='utf-8') as f:
        raw_data = [json.loads(line) for line in f]
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(raw_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ –ü–û–õ–ù–û–ì–û –¥–∞—Ç–∞—Å–µ—Ç–∞")
    
    # Format for MLX training
    mlx_data = []
    for item in raw_data:
        formatted = {
            "text": f"<bos><start_of_turn>user\n{item['instruction']}<end_of_turn>\n<start_of_turn>model\n{item['output']}<end_of_turn><eos>"
        }
        mlx_data.append(formatted)
    
    # Split into train/validation (90/10)
    train_data, valid_data = train_test_split(mlx_data, test_size=0.1, random_state=42)
    
    # Save training data
    with open("data/train.jsonl", 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    # Save validation data
    with open("data/valid.jsonl", 'w', encoding='utf-8') as f:
        for item in valid_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(train_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, {len(valid_data)} –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    print(f"üìà –£–≤–µ–ª–∏—á–µ–Ω–∏–µ: —Å 7786 –¥–æ {len(train_data)} (+{len(train_data)-7786} –æ–±—Ä–∞–∑—Ü–æ–≤)")
    return True

def run_full_training():
    """Run full training with extended parameters"""
    print("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø...")
    print("=" * 60)
    
    # Training parameters for FULL dataset
    MODEL_NAME = "google/gemma-3-4b-it"
    DATA_PATH = "data"
    BATCH_SIZE = 1
    ITERS = 3000  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    SAVE_EVERY = 200  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —á–∞—â–µ
    LEARNING_RATE = 1e-4  # –ß—É—Ç—å –º–µ–Ω—å—à–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    GRAD_CHECKPOINT = True
    
    print(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å: {MODEL_NAME}")
    print(f"   ‚Ä¢ –ò—Ç–µ—Ä–∞—Ü–∏–π: {ITERS}")
    print(f"   ‚Ä¢ Learning Rate: {LEARNING_RATE}")
    print(f"   ‚Ä¢ Batch Size: {BATCH_SIZE}")
    print(f"   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ: {SAVE_EVERY} –∏—Ç–µ—Ä–∞—Ü–∏–π")
    print(f"   ‚Ä¢ –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã: {'–î–∞' if GRAD_CHECKPOINT else '–ù–µ—Ç'}")
    print()
    
    # Build MLX command
    train_cmd = f"TOKENIZERS_PARALLELISM=false python -m mlx_lm.lora " \
                f"--model {MODEL_NAME} " \
                f"--train " \
                f"--iters {ITERS} " \
                f"--data {DATA_PATH} " \
                f"--adapter-path ./checkpoints_full " \
                f"--save-every {SAVE_EVERY} " \
                f"--batch-size {BATCH_SIZE} " \
                f"--learning-rate {LEARNING_RATE} "
    
    if GRAD_CHECKPOINT:
        train_cmd += "--grad-checkpoint "
    
    print("üñ•Ô∏è  –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:")
    print(train_cmd)
    print()
    
    try:
        # Create checkpoints directory
        os.makedirs("checkpoints_full", exist_ok=True)
        
        # Run training
        result = subprocess.run(train_cmd, shell=True, check=True)
        
        print("\\n‚úÖ –ü–û–õ–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print("üìÅ –ê–¥–∞–ø—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./checkpoints_full/")
        
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
        return False

def estimate_training_time():
    """Estimate training time for full dataset"""
    print("‚è±Ô∏è  –û–¶–ï–ù–ö–ê –í–†–ï–ú–ï–ù–ò –û–ë–£–ß–ï–ù–ò–Ø:")
    print("=" * 50)
    
    # Based on previous runs: ~0.8 it/sec, 185 tokens/sec
    total_iterations = 3000
    seconds_per_iteration = 1.25  # Conservative estimate
    
    total_seconds = total_iterations * seconds_per_iteration
    hours = total_seconds / 3600
    
    print(f"üìä –ü–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: 8652 –∑–∞–ø–∏—Å–∏")
    print(f"üîÑ –ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏: {total_iterations}")
    print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: ~0.8 –∏—Ç–µ—Ä–∞—Ü–∏–π/—Å–µ–∫")
    print(f"‚è∞ –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: {hours:.1f} —á–∞—Å–æ–≤ ({hours*60:.0f} –º–∏–Ω—É—Ç)")
    print(f"üïê –ù–∞—á–∞–ª–æ: —Å–µ–π—á–∞—Å")
    print(f"üïê –û–∫–æ–Ω—á–∞–Ω–∏–µ: —á–µ—Ä–µ–∑ ~{hours:.1f}—á")
    print()
    
    response = input("ü§î –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ? (y/n): ").strip().lower()
    return response in ['y', 'yes', '–¥–∞', '']

def main():
    """Main function"""
    print("üèõÔ∏è –ü–û–õ–ù–û–ï –î–û–û–ë–£–ß–ï–ù–ò–ï GEMMA 3 4B")
    print("üåü –ù–ê –í–°–Å–ú –î–ê–¢–ê–°–ï–¢–ï –ö–ê–ó–ê–•–°–¢–ê–ù–°–ö–û–ì–û –ü–†–ê–í–ê")
    print("=" * 60)
    
    # Check MLX installation
    try:
        import mlx_lm
        print("‚úÖ MLX —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except ImportError:
        print("‚ùå MLX –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install mlx-lm")
        return False
    
    # Estimate time
    if not estimate_training_time():
        print("üö´ –û–±—É—á–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return False
    
    # Prepare full dataset
    if not prepare_full_dataset():
        print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        return False
    
    # Run full training
    success = run_full_training()
    
    if success:
        print("\\nüéâ –£–°–ü–ï–•! –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print("üß™ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å:")
        print("   python test_gemma_4b.py")
        print("   # –£–∫–∞–∂–∏—Ç–µ –Ω–æ–≤—ã–π –ø—É—Ç—å: checkpoints_full")
    
    return success

if __name__ == "__main__":
    main() 