#!/usr/bin/env python3
"""
Monitor training progress and test model after completion
"""

import os
import time
import subprocess
from datetime import datetime

def check_training_status():
    """Check if training is still running"""
    try:
        result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
        return 'mlx_lm' in result.stdout
    except:
        return False

def get_latest_checkpoint():
    """Get the latest checkpoint directory"""
    if not os.path.exists('checkpoints_full'):
        return None
    
    # MLX saves final adapters directly
    if os.path.exists('checkpoints_full/adapters.safetensors'):
        return 'checkpoints_full'
    
    # Look for numbered checkpoints
    files = os.listdir('checkpoints_full')
    checkpoint_files = [f for f in files if f.endswith('_adapters.safetensors')]
    if not checkpoint_files:
        return None
    
    # Get latest by number
    latest_file = max(checkpoint_files, key=lambda x: int(x.split('_')[0]))
    return f"checkpoints_full"  # MLX loads from directory, not specific file

def test_model():
    """Test the trained model"""
    print("üß™ **–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò**")
    
    try:
        from mlx_lm import load, generate
        
        checkpoint_path = get_latest_checkpoint()
        if not checkpoint_path:
            print("‚ùå –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å —á–µ–∫–ø–æ–∏–Ω—Ç–æ–º: {checkpoint_path}")
        
        model, tokenizer = load(
            "google/gemma-3-4b-it",
            adapter_path=checkpoint_path
        )
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        
        # Test questions
        questions = [
            "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¢–û–û –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ?",
            "–ö–∞–∫–æ–≤–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è —Ç—Ä—É–¥–æ–≤–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞ –≤ –†–ö?",
            "–ö–∞–∫–∏–µ –Ω–∞–ª–æ–≥–∏ –æ–±—è–∑–∞–Ω–æ –ø–ª–∞—Ç–∏—Ç—å –¢–û–û –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ?",
            "–ö—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–º –¢–û–û –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ?"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\n{i}. üìù –í–æ–ø—Ä–æ—Å: {question}")
            
            prompt = f"<bos><start_of_turn>user\n{question}<end_of_turn>\n<start_of_turn>model\n"
            
            response = generate(
                model, 
                tokenizer, 
                prompt=prompt, 
                max_tokens=200
            )
            
            print(f"ü§ñ –û—Ç–≤–µ—Ç: {response}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

def monitor_training():
    """Monitor training progress"""
    print("üîç **–ú–û–ù–ò–¢–û–†–ò–ù–ì –û–ë–£–ß–ï–ù–ò–Ø**")
    print(f"‚è∞ –ù–∞—á–∞–ª–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {datetime.now().strftime('%H:%M:%S')}")
    
    while True:
        if check_training_status():
            # Check for new checkpoints
            checkpoint = get_latest_checkpoint()
            if checkpoint:
                print(f"üìä –ê–∫—Ç–∏–≤–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint} | {datetime.now().strftime('%H:%M:%S')}")
            else:
                print(f"üîÑ –û–±—É—á–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ... | {datetime.now().strftime('%H:%M:%S')}")
            
            time.sleep(300)  # Check every 5 minutes
        else:
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! | {datetime.now().strftime('%H:%M:%S')}")
            break
    
    # Wait a bit for files to finalize
    time.sleep(10)
    
    # Test the model
    test_model()

if __name__ == "__main__":
    monitor_training() 