# üîå API –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ Gemma 3 4B

## üìö –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

### load() - –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
```python
from mlx_lm import load

model, tokenizer = load(
    path="google/gemma-3-4b-it",           # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
    adapter_path="./checkpoints_full",      # –ü—É—Ç—å –∫ –∞–¥–∞–ø—Ç–µ—Ä–∞–º
    tokenizer_config={}                     # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `path`: –ü—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (—Å—Ç—Ä–æ–∫–∞)
- `adapter_path`: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω—ã–º –∞–¥–∞–ø—Ç–µ—Ä–∞–º (—Å—Ç—Ä–æ–∫–∞)
- `tokenizer_config`: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (—Å–ª–æ–≤–∞—Ä—å)

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `model`: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
- `tokenizer`: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä

### generate() - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
```python
from mlx_lm import generate

response = generate(
    model=model,                    # –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    tokenizer=tokenizer,           # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    prompt="–í–∞—à –ø—Ä–æ–º–ø—Ç",           # –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
    max_tokens=200,                # –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
    temp=0.1,                      # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    top_p=0.9,                     # Nucleus sampling
    repetition_penalty=1.0,        # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã
    stream=False,                  # –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    verbose=False                  # –î–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `model`: –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π)
- `tokenizer`: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π)  
- `prompt`: –í—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–º–ø—Ç (—Å—Ç—Ä–æ–∫–∞)
- `max_tokens`: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (int, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100)
- `temp`: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ (float, 0.0-2.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.0)
- `top_p`: Nucleus sampling –ø–∞—Ä–∞–º–µ—Ç—Ä (float, 0.0-1.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1.0)
- `repetition_penalty`: –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã (float, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1.0)
- `stream`: –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: False)
- `verbose`: –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ (bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: False)

**–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:**
- `str`: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç

## üéõÔ∏è –§–æ—Ä–º–∞—Ç—ã –ø—Ä–æ–º–ø—Ç–æ–≤

### –ë–∞–∑–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç Gemma
```python
prompt = f"""<bos><start_of_turn>user
{–≤–∞—à_–≤–æ–ø—Ä–æ—Å}<end_of_turn>
<start_of_turn>model
"""
```

### –ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
```python
def format_legal_prompt(question, context=None):
    if context:
        prompt = f"""<bos><start_of_turn>user
–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

–í–æ–ø—Ä–æ—Å: {question}<end_of_turn>
<start_of_turn>model
"""
    else:
        prompt = f"""<bos><start_of_turn>user
{question}<end_of_turn>
<start_of_turn>model
"""
    return prompt
```

### –ü–æ—à–∞–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
```python
def format_step_by_step_prompt(question):
    prompt = f"""<bos><start_of_turn>user
–û–±—ä—è—Å–Ω–∏ –ø–æ—à–∞–≥–æ–≤–æ: {question}

–¢—Ä–µ–±—É–µ—Ç—Å—è:
1. –ö—Ä–∞—Ç–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
2. –ü–æ—à–∞–≥–æ–≤–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞  
3. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã
4. –°—Å—ã–ª–∫–∏ –Ω–∞ –∑–∞–∫–æ–Ω—ã<end_of_turn>
<start_of_turn>model
"""
    return prompt
```

## üîß –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

### ask_legal_question() - –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
```python
def ask_legal_question(question, model, tokenizer, max_tokens=300, temp=0.1):
    """
    –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ –∫–∞–∑–∞—Ö—Å–∫–æ–º—É –ø—Ä–∞–≤—É
    
    Args:
        question (str): –ü—Ä–∞–≤–æ–≤–æ–π –≤–æ–ø—Ä–æ—Å
        model: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        max_tokens (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        temp (float): –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
    Returns:
        str: –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
    """
    prompt = format_legal_prompt(question)
    
    response = generate(
        model, tokenizer,
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temp
    )
    
    # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
    if "<end_of_turn>" in response:
        response = response.split("<end_of_turn>")[0]
    
    return response.strip()
```

### batch_process() - –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
```python
def batch_process(questions, model, tokenizer, max_tokens=200):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
    
    Args:
        questions (list): –°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤
        model: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        max_tokens (int): –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤
    """
    results = []
    for i, question in enumerate(questions):
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}/{len(questions)}: {question[:50]}...")
        
        answer = ask_legal_question(
            question, model, tokenizer, max_tokens
        )
        
        results.append({
            "question": question,
            "answer": answer,
            "timestamp": time.time()
        })
    
    return results
```

### save_conversation() - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞
```python
import json
from datetime import datetime

def save_conversation(conversation, filename=None):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞ –≤ JSON —Ñ–∞–π–ª
    
    Args:
        conversation (list): –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        filename (str): –ò–º—è —Ñ–∞–π–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        str: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"conversation_{timestamp}.json"
    
    data = {
        "timestamp": datetime.now().isoformat(),
        "model": "gemma-3-4b-legal-kz",
        "conversation": conversation
    }
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filename
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –º–µ—Ç—Ä–∏–∫–∏

### performance_monitor() - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
```python
import time
import psutil

class PerformanceMonitor:
    def __init__(self):
        self.start_time = None
        self.start_memory = None
        
    def start(self):
        """–ù–∞—á–∞—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
        self.start_time = time.time()
        self.start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
    def stop(self):
        """–ó–∞–∫–æ–Ω—á–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –≤–µ—Ä–Ω—É—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        return {
            "duration": end_time - self.start_time,
            "memory_start": self.start_memory,
            "memory_end": end_memory,
            "memory_peak": end_memory - self.start_memory
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
monitor = PerformanceMonitor()
monitor.start()
response = ask_legal_question("–ß—Ç–æ —Ç–∞–∫–æ–µ –¢–û–û?", model, tokenizer)
metrics = monitor.stop()

print(f"–í—Ä–µ–º—è: {metrics['duration']:.2f} —Å–µ–∫")
print(f"–ü–∞–º—è—Ç—å: {metrics['memory_peak']:.1f} MB")
```

### calculate_tokens() - –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
```python
def calculate_tokens(text, tokenizer):
    """
    –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
    
    Args:
        text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        
    Returns:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
    """
    tokens = tokenizer.encode(text)
    return len(tokens)

def estimate_cost(prompt, max_tokens, tokenizer):
    """
    –û—Ü–µ–Ω–∫–∞ "—Å—Ç–æ–∏–º–æ—Å—Ç–∏" –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    
    Args:
        prompt (str): –í—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–º–ø—Ç
        max_tokens (int): –ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç–≤–µ—Ç–∞
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        
    Returns:
        dict: –û—Ü–µ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤
    """
    input_tokens = calculate_tokens(prompt, tokenizer)
    total_tokens = input_tokens + max_tokens
    
    # –û—Ü–µ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
    estimated_time = total_tokens / 7.3  # 7.3 —Ç–æ–∫–µ–Ω/—Å–µ–∫
    estimated_memory = 9 + (total_tokens * 0.001)  # ~1MB –Ω–∞ 1000 —Ç–æ–∫–µ–Ω–æ–≤
    
    return {
        "input_tokens": input_tokens,
        "max_output_tokens": max_tokens,
        "total_tokens": total_tokens,
        "estimated_time": estimated_time,
        "estimated_memory_mb": estimated_memory
    }
```

## üîÑ –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

### stream_generate() - –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
```python
def stream_generate(question, model, tokenizer, max_tokens=300):
    """
    –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –≤—ã–≤–æ–¥–æ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    
    Args:
        question (str): –í–æ–ø—Ä–æ—Å
        model: –ú–æ–¥–µ–ª—å
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        max_tokens (int): –ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤
        
    Yields:
        str: –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞
    """
    prompt = format_legal_prompt(question)
    
    # –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
    for token in generate(
        model, tokenizer,
        prompt=prompt,
        max_tokens=max_tokens,
        temp=0.1,
        stream=True
    ):
        yield token

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
print("–û—Ç–≤–µ—Ç:", end=" ", flush=True)
for token in stream_generate("–ß—Ç–æ —Ç–∞–∫–æ–µ –¢–û–û?", model, tokenizer):
    print(token, end="", flush=True)
print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –∫–æ–Ω—Ü–µ
```

## üéØ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

### legal_definitions() - –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
```python
def get_legal_definition(term, model, tokenizer):
    """–ü–æ–ª—É—á–∏—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞"""
    prompt = f"""<bos><start_of_turn>user
–î–∞–π —Ç–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞: {term}

–¢—Ä–µ–±—É–µ—Ç—Å—è:
- –ö—Ä–∞—Ç–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –ò—Å—Ç–æ—á–Ω–∏–∫ –≤ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–º –ø—Ä–∞–≤–µ
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ<end_of_turn>
<start_of_turn>model
"""
    
    return generate(
        model, tokenizer,
        prompt=prompt,
        max_tokens=200,
        temp=0.05  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    )
```

### legal_procedure() - –ü—Ä–∞–≤–æ–≤—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã
```python
def get_legal_procedure(procedure, model, tokenizer):
    """–ü–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∞–≤–æ–≤–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã"""
    prompt = f"""<bos><start_of_turn>user
–û–ø–∏—à–∏ –ø–æ—à–∞–≥–æ–≤–æ –ø—Ä–æ—Ü–µ–¥—É—Ä—É: {procedure}

–ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å:
1. –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
2. –ü–æ—à–∞–≥–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
3. –°—Ä–æ–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
4. –í–æ–∑–º–æ–∂–Ω—ã–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏<end_of_turn>
<start_of_turn>model
"""
    
    return generate(
        model, tokenizer,
        prompt=prompt,
        max_tokens=400,
        temp=0.1
    )
```

## üì± –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–±-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏

### Flask API
```python
from flask import Flask, request, jsonify

app = Flask(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ)
model, tokenizer = load(
    "google/gemma-3-4b-it",
    adapter_path="./checkpoints_full"
)

@app.route('/ask', methods=['POST'])
def api_ask():
    data = request.json
    question = data.get('question')
    max_tokens = data.get('max_tokens', 200)
    
    if not question:
        return jsonify({'error': '–í–æ–ø—Ä–æ—Å –Ω–µ —É–∫–∞–∑–∞–Ω'}), 400
    
    try:
        answer = ask_legal_question(question, model, tokenizer, max_tokens)
        return jsonify({
            'question': question,
            'answer': answer,
            'model': 'gemma-3-4b-legal-kz'
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy', 'model_loaded': model is not None})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### FastAPI (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π)
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="Gemma Legal API", version="1.0.0")

class QuestionRequest(BaseModel):
    question: str
    max_tokens: int = 200
    temperature: float = 0.1

class AnswerResponse(BaseModel):
    question: str
    answer: str
    model: str = "gemma-3-4b-legal-kz"

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
@app.on_event("startup")
async def load_model():
    global model, tokenizer
    model, tokenizer = load(
        "google/gemma-3-4b-it",
        adapter_path="./checkpoints_full"
    )

@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    try:
        answer = ask_legal_question(
            request.question, 
            model, 
            tokenizer, 
            request.max_tokens,
            request.temperature
        )
        return AnswerResponse(question=request.question, answer=answer)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

**üìñ –°–æ–≤–µ—Ç:** –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–∏–∑–∫—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É (0.05-0.1) –¥–ª—è –ø—Ä–∞–≤–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤! 